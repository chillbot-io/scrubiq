# scrubIQ Training Plan

**Goal:** Train a TP/FP classifier to reduce false positives and improve detection accuracy beyond what regex + Presidio can achieve.

**Target Headline:** "scrubIQ detects 23% more PHI than Presidio with 31% fewer false positives"

---

## Overview

The training pipeline has three components:

1. **Training Data** - Curated datasets with ground-truth labels
2. **Model Training** - SetFit classifier for TP/FP prediction
3. **Evaluation** - Benchmarks against Presidio and other baselines

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  Training Data  │ ──▶ │  Train Model    │ ──▶ │  Benchmark      │
│  - i2b2 2014    │     │  - SetFit       │     │  - vs Presidio  │
│  - Synthetic    │     │  - ~100MB       │     │  - vs Baseline  │
│  - User Reviews │     │  - CPU training │     │  - Publish      │
└─────────────────┘     └─────────────────┘     └─────────────────┘
```

---

## Part 1: Training Data

### 1.1 Primary Dataset: Nemotron-CC-PII

**Source:** NVIDIA Nemotron-CC dataset with PII annotations  
**Size:** ~500K examples  
**Cost:** Free (requires HuggingFace account)  
**Link:** https://huggingface.co/datasets/nvidia/Nemotron-CC

This is synthetic data generated by Nemotron-4 340B with PII labels. Categories include:
- Names (PERSON)
- Addresses (LOCATION, ADDRESS)
- Phone numbers
- Email addresses
- SSN/national IDs
- Credit card numbers
- Medical record numbers
- Dates of birth

**How to download:**
```bash
pip install datasets
python -c "
from datasets import load_dataset
ds = load_dataset('nvidia/Nemotron-CC', split='train')
ds.save_to_disk('./training/datasets/nemotron')
"
```

### 1.2 Gold Standard: i2b2 2014 De-identification Challenge

**Source:** Harvard DBMI  
**Size:** ~1,304 clinical notes with expert PHI annotations  
**Cost:** Free (requires data use agreement, ~1 week approval)  
**Link:** https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/

This is the gold standard for PHI detection benchmarks. Academic papers use this to report accuracy numbers.

**Apply for access:**
1. Go to https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/
2. Create account, submit data use agreement
3. Wait for approval (~5-7 business days)
4. Download 2014 De-identification track data

**i2b2 entity types:**
- NAME
- PROFESSION
- LOCATION
- AGE
- DATE
- CONTACT
- ID (SSN, MRN, etc.)
- MEDICALRECORD
- HEALTHPLAN

### 1.3 Synthetic Data (Generated)

We've already built a synthetic data generator in `scripts/generate_test_corpus.py`. This creates documents with planted PII where we know the ground truth.

**Categories:**
- HR documents (SSN, DOB, addresses)
- Finance documents (credit cards, bank accounts)
- Medical documents (MRN, health plan IDs, diagnoses)
- Clean documents (no PII)
- Test data documents (obvious fake data like 123-45-6789)

**Generate:**
```bash
python scripts/generate_test_corpus.py ./training/datasets/synthetic --count 1000
```

### 1.4 User Feedback (Post-Launch)

After launch, users can review low-confidence matches and provide feedback:
- `scrubiq review <scan_id>` - TUI for reviewing matches
- Feedback stored in `~/.config/scrubiq/feedback/reviews.jsonl`
- Can retrain local model with `scrubiq model retrain`

---

## Part 2: Training Data Format

### 2.1 Standard Format

All training data should be converted to this JSONL format:

```json
{
  "text": "Employee [NAME] (SSN: [SSN]) enrolled in benefits.",
  "entity_type": "ssn",
  "entity_position": [21, 26],
  "verdict": "TP",
  "source": "i2b2",
  "doc_type": "clinical",
  "confidence": 0.85,
  "detector": "regex",
  "reason": null
}
```

**Fields:**
- `text` - Context with entity replaced by `[TYPE]` token
- `entity_type` - Our entity type (ssn, name, phone, etc.)
- `entity_position` - Start/end of the `[TYPE]` token in text
- `verdict` - "TP" (true positive) or "FP" (false positive)
- `source` - Where data came from (i2b2, nemotron, synthetic, user)
- `doc_type` - Document category (clinical, hr, finance, email, etc.)
- `confidence` - Original detector confidence (if available)
- `detector` - Which detector found it (regex, presidio)
- `reason` - Why FP if applicable (test_data, wrong_format, business_number)

### 2.2 Entity Type Mapping

Map source dataset types to our types:

| i2b2 Type | Nemotron Type | Our Type |
|-----------|---------------|----------|
| NAME | PERSON | name |
| LOCATION | ADDRESS | address |
| CONTACT | PHONE | phone |
| ID | SSN | ssn |
| MEDICALRECORD | MEDICAL_ID | mrn |
| HEALTHPLAN | HEALTH_PLAN | health_plan_id |
| DATE | DATE_OF_BIRTH | dob |
| AGE | AGE | dob |
| - | CREDIT_CARD | credit_card |
| - | EMAIL | email |

### 2.3 Data Loading Scripts

**Load i2b2:**
```python
# training/load_i2b2.py
from pathlib import Path
import xml.etree.ElementTree as ET

def load_i2b2_file(xml_path: Path) -> list[dict]:
    """Load single i2b2 XML file."""
    tree = ET.parse(xml_path)
    root = tree.getroot()
    
    text = root.find(".//TEXT").text
    tags = root.findall(".//TAGS/*") or root.findall(".//*[@TYPE]")
    
    examples = []
    for tag in tags:
        if tag.get("TYPE"):
            # Extract entity info
            phi_type = tag.get("TYPE")
            start = int(tag.get("start", 0))
            end = int(tag.get("end", 0))
            
            # Map to our type
            our_type = I2B2_TYPE_MAP.get(phi_type)
            if not our_type:
                continue
            
            # Create training example
            examples.append({
                "text": extract_context(text, start, end, our_type),
                "entity_type": our_type,
                "verdict": "TP",  # i2b2 annotations are ground truth
                "source": "i2b2",
                "doc_type": "clinical",
            })
    
    return examples
```

**Load Nemotron:**
```python
# training/load_nemotron.py
from datasets import load_dataset

def load_nemotron(n_examples: int = 10000) -> list[dict]:
    """Load subset of Nemotron-CC with PII."""
    ds = load_dataset('nvidia/Nemotron-CC', split='train', streaming=True)
    
    examples = []
    for item in ds.take(n_examples):
        if item.get('pii_labels'):
            for label in item['pii_labels']:
                examples.append({
                    "text": extract_context(item['text'], label['start'], label['end'], label['type']),
                    "entity_type": map_nemotron_type(label['type']),
                    "verdict": "TP",
                    "source": "nemotron",
                    "doc_type": infer_doc_type(item['text']),
                })
    
    return examples
```

---

## Part 3: Model Architecture

### 3.1 Why SetFit?

SetFit is ideal for this use case because:
- Works with small datasets (100-1000 examples)
- Trains on CPU in minutes
- ~100MB model size
- No GPU required
- 90%+ accuracy on text classification tasks

**How it works:**
1. Uses sentence-transformers to embed text
2. Fine-tunes embeddings with contrastive learning
3. Trains a classification head on top

### 3.2 Model Input Format

The model takes formatted text as input:

```
[SSN] in context: Employee John Smith (SSN: [SSN]) has been approved for...
```

Format: `[ENTITY_TYPE] in context: {surrounding text with entity replaced by token}`

### 3.3 Model Output

Binary classification:
- 1 = True Positive (real sensitive data)
- 0 = False Positive (test data, wrong format, business info)

### 3.4 Training Script

```python
# training/train_model.py
from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset

def train_tpfp_model(
    train_data: list[dict],
    output_dir: str = "models/tpfp-v1",
    base_model: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> SetFitModel:
    """Train TP/FP classifier."""
    
    # Format inputs
    texts = [f"[{d['entity_type'].upper()}] in context: {d['text']}" for d in train_data]
    labels = [1 if d['verdict'] == 'TP' else 0 for d in train_data]
    
    # Create dataset
    dataset = Dataset.from_dict({"text": texts, "label": labels})
    
    # Train/test split
    split = dataset.train_test_split(test_size=0.2)
    
    # Initialize model
    model = SetFitModel.from_pretrained(base_model)
    
    # Train
    trainer = SetFitTrainer(
        model=model,
        train_dataset=split["train"],
        eval_dataset=split["test"],
        num_iterations=20,
        num_epochs=1,
    )
    trainer.train()
    
    # Save
    model.save_pretrained(output_dir)
    
    # Evaluate
    metrics = trainer.evaluate()
    print(f"Accuracy: {metrics['accuracy']:.1%}")
    
    return model
```

### 3.5 Integration with Pipeline

```python
# src/scrubiq/classifier/pipeline.py

class ClassifierPipeline:
    def __init__(self, ..., enable_tpfp_model: bool = True, tpfp_model_path: str = None):
        # ... existing init ...
        
        self.tpfp_model = None
        if enable_tpfp_model:
            self.tpfp_model = self._load_tpfp_model(tpfp_model_path)
    
    def _load_tpfp_model(self, path):
        """Load TP/FP model if available."""
        if path and Path(path).exists():
            from setfit import SetFitModel
            return SetFitModel.from_pretrained(path)
        
        # Try bundled model
        bundled = Path(__file__).parent / "models" / "tpfp-v1"
        if bundled.exists():
            from setfit import SetFitModel
            return SetFitModel.from_pretrained(bundled)
        
        return None
    
    def classify(self, text: str, filename: str = "") -> ClassificationResult:
        # ... existing detection ...
        
        # Filter with TP/FP model
        if self.tpfp_model:
            matches = self._filter_with_model(matches, text)
        
        return ClassificationResult(matches=matches, ...)
    
    def _filter_with_model(self, matches: list[Match], text: str) -> list[Match]:
        """Use TP/FP model to filter false positives."""
        filtered = []
        for match in matches:
            input_text = f"[{match.entity_type.value.upper()}] in context: {match.context}"
            prediction = self.tpfp_model.predict([input_text])[0]
            
            if prediction == 1:  # True positive
                filtered.append(match)
            else:
                # Mark as test data / false positive
                match.is_test_data = True
                filtered.append(match)  # Keep but flag
        
        return filtered
```

---

## Part 4: Benchmarking

### 4.1 Metrics

Report these metrics for each benchmark:

| Metric | Formula | Meaning |
|--------|---------|---------|
| Precision | TP / (TP + FP) | Of detected items, how many were real |
| Recall | TP / (TP + FN) | Of real items, how many were detected |
| F1 | 2 * P * R / (P + R) | Harmonic mean of precision and recall |
| FP Rate | FP / (FP + TN) | False alarm rate |

### 4.2 Baseline Comparisons

Compare against:

1. **Regex only** - Our current regex patterns without NER
2. **Presidio default** - Presidio with default settings
3. **Presidio + regex** - Our combined pipeline without TP/FP model
4. **scrubIQ full** - Everything including TP/FP model

### 4.3 Benchmark Script

```bash
# Run benchmark
python scripts/benchmark.py ./test_corpus --compare-presidio

# Output example:
# ======================================================================
# COMPARISON
# ======================================================================
# 
# Metric                         With Presidio      Regex Only
# ----------------------------------------------------------------------
# Precision                              87.3%           100.0%
# Recall                                 94.1%            85.3%
# F1 Score                               90.6%            92.1%
# ----------------------------------------------------------------------
```

### 4.4 i2b2 Benchmark (Gold Standard)

For publishable numbers, benchmark on i2b2 2014:

```bash
# After downloading i2b2 data
python scripts/benchmark_i2b2.py ./data/i2b2-2014 --output benchmark_results.json
```

Report format for README/marketing:
```
Dataset: i2b2 2014 De-identification Challenge (1,304 clinical notes)

| System | Precision | Recall | F1 |
|--------|-----------|--------|-----|
| Presidio | 82.3% | 76.4% | 79.2% |
| scrubIQ (regex) | 91.2% | 71.3% | 80.0% |
| scrubIQ (full) | 89.5% | 87.1% | 88.3% |

scrubIQ achieves 11.5% higher F1 score than Presidio on PHI detection.
```

---

## Part 5: Training Schedule

### Week 1: Data Preparation

- [ ] Apply for i2b2 data access
- [ ] Download Nemotron-CC dataset
- [ ] Generate 1000 synthetic documents
- [ ] Write data loading scripts
- [ ] Convert all data to standard JSONL format

### Week 2: Initial Training

- [ ] Install SetFit and dependencies
- [ ] Train initial model on synthetic data
- [ ] Evaluate on held-out synthetic test set
- [ ] Iterate on training parameters

### Week 3: i2b2 Training

- [ ] (Assuming i2b2 access approved)
- [ ] Load and convert i2b2 data
- [ ] Fine-tune model on clinical notes
- [ ] Run i2b2 benchmark
- [ ] Document accuracy numbers

### Week 4: Integration & Polish

- [ ] Integrate model into pipeline
- [ ] Bundle model with package (~100MB)
- [ ] Update benchmarks in README
- [ ] Test end-to-end with model
- [ ] Ship v1.1.0 with trained model

---

## Part 6: Post-Launch: Continuous Improvement

### 6.1 User Feedback Loop

```
User scans → Detects matches → User reviews → Feedback stored → Model improves
                                      ↓
                              (opt-in) Telemetry
                                      ↓
                              Central model improves
                                      ↓
                              All users benefit
```

### 6.2 Local Retraining

Users can improve their local model with their feedback:

```bash
# After reviewing ~50 matches
scrubiq model retrain

# Check model info
scrubiq model info
# Output:
# Base model:    v1.0.0 (shipped with scrubIQ)
# Local model:   v1.0.0+local.3
# Training data: 127 reviews (94 TP, 33 FP)
```

### 6.3 Telemetry (Opt-in)

If users enable telemetry:
- Anonymized feedback (no PII values, just context patterns)
- Used to improve central model
- New versions ship with better model

```bash
# Enable telemetry
scrubiq config set telemetry.enabled true

# Disable
scrubiq config set telemetry.enabled false
```

---

## Part 7: Files Structure

```
scrubiq/
├── training/
│   ├── datasets/
│   │   ├── i2b2/              # Downloaded i2b2 data
│   │   │   ├── raw/
│   │   │   └── processed/
│   │   ├── nemotron/          # Nemotron-CC subset
│   │   ├── synthetic/         # Generated test docs
│   │   └── feedback/          # User reviews
│   ├── scripts/
│   │   ├── load_i2b2.py
│   │   ├── load_nemotron.py
│   │   ├── train_model.py
│   │   └── benchmark_i2b2.py
│   └── models/
│       ├── tpfp-v1/           # Shipped with package
│       └── tpfp-v1+local.N/   # User's fine-tuned
├── scripts/
│   ├── generate_test_corpus.py
│   └── benchmark.py
└── src/scrubiq/
    ├── classifier/
    │   ├── models/
    │   │   └── tpfp-v1/       # Bundled model
    │   └── pipeline.py        # Uses model
    └── training/
        ├── data.py            # Data loading utilities
        └── model.py           # Training utilities
```

---

## Appendix: Quick Start Commands

```bash
# 1. Generate synthetic training data
python scripts/generate_test_corpus.py ./training/datasets/synthetic --count 1000

# 2. Run current benchmark (no training needed)
python scripts/benchmark.py ./test_corpus

# 3. Install training dependencies
pip install setfit datasets sentence-transformers

# 4. Train model (after preparing data)
python training/scripts/train_model.py \
    --data ./training/datasets \
    --output ./training/models/tpfp-v1

# 5. Benchmark trained model
python scripts/benchmark.py ./test_corpus --model ./training/models/tpfp-v1

# 6. Package model with release
cp -r ./training/models/tpfp-v1 ./src/scrubiq/classifier/models/
```

---

## Appendix: Expected Results

Based on similar systems and our current regex baseline:

| Configuration | Precision | Recall | F1 |
|--------------|-----------|--------|-----|
| Regex only | ~95% | ~85% | ~90% |
| + Presidio NER | ~87% | ~92% | ~89% |
| + TP/FP model | ~92% | ~90% | ~91% |
| vs Presidio alone | - | - | +10-15% |

The TP/FP model should:
- Reduce false positives by 30-50%
- Maintain or slightly improve recall
- Net F1 improvement of 5-15% over baseline

---

*Training Plan by Claude • December 2024*
